{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36358e8d-72df-4b51-a44a-78c560533f0b",
   "metadata": {},
   "source": [
    "# 抽出型の要約の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a0c5ecc-6a84-4234-8ed1-2f9347c0124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumeval.metrics.bleu import BLEUCalculator\n",
    "from sumeval.metrics.rouge import RougeCalculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c62e9ad-1b75-4e67-bdf9-fd8aeb0f6f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "pd.set_option('max_rows', 1000)\n",
    "pd.set_option('max_columns', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25106a73-b84f-47c4-9ec6-d19b4ee2672e",
   "metadata": {},
   "source": [
    "## パラメータの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47f20041-1c3b-4eb7-a5df-6a9d09af7310",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_path = Path('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ca6c79-8da6-43e6-9326-00d3543cc118",
   "metadata": {},
   "source": [
    "## 要約データの取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eec6b15-3a20-4f2b-ba52-3b64d93b1f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "body_data = pd.read_csv(data_dir_path.joinpath('body_data.csv'))\n",
    "summary_data = pd.read_csv(data_dir_path.joinpath('summary_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a41afd32-fc21-4124-bea3-4b16d9dad6b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11036614</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11489315</td>\n",
       "      <td>太陽170億個分もの超大質量ブラックホールがありえない場所で発見される</td>\n",
       "      <td>\\nこれまで発見された超大質量のブラックホールは、いずれも「銀河の密集地帯」から発見されてい...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11560663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11481499</td>\n",
       "      <td>「一緒に帰ろう」はNG！気になる彼と仲良くなるための4つの鉄則</td>\n",
       "      <td>\\n気になる男子と仲良くなろうと思って、勇気を振り絞って「今日、一緒に帰ろうよ」と言ったとこ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11545109</td>\n",
       "      <td>「40代以降で結婚できる人は1.2％」厳しすぎるアラフォーの婚活事情</td>\n",
       "      <td>\\n「40代以降で結婚できる男性は1.2％、女性は2.7％」(※)。この数字を見て、心が折れ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                title  \\\n",
       "0    11036614                                  NaN   \n",
       "1    11489315  太陽170億個分もの超大質量ブラックホールがありえない場所で発見される   \n",
       "2    11560663                                  NaN   \n",
       "3    11481499      「一緒に帰ろう」はNG！気になる彼と仲良くなるための4つの鉄則   \n",
       "4    11545109   「40代以降で結婚できる人は1.2％」厳しすぎるアラフォーの婚活事情   \n",
       "\n",
       "                                                text  \n",
       "0                                                NaN  \n",
       "1  \\nこれまで発見された超大質量のブラックホールは、いずれも「銀河の密集地帯」から発見されてい...  \n",
       "2                                                NaN  \n",
       "3  \\n気になる男子と仲良くなろうと思って、勇気を振り絞って「今日、一緒に帰ろうよ」と言ったとこ...  \n",
       "4  \\n「40代以降で結婚できる男性は1.2％、女性は2.7％」(※)。この数字を見て、心が折れ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17001291-32b3-4870-9141-e872fe19cabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "body_data = body_data.query('text.notnull()', engine='python').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41e2643c-8fe4-486e-937b-9958a5ac69d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = pd.merge(\n",
    "    summary_data,\n",
    "    body_data[['article_id']],\n",
    "    on='article_id', how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f230885-66b9-4930-93fc-ae269982990e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13462, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5748e1-9e1a-4535-9727-fae505497049",
   "metadata": {},
   "source": [
    "## ニュースの要約"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22969569-d959-40d0-9b73-f77467cb4f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「気になる彼と仲良くなるための鉄則」を、恋愛上手な女子に聞きました！\n",
      "「一緒に帰ろう」より「一緒に行こう」と誘い、共通の目的を持つといいそう\n",
      "過去の話をして、共通点を見出すのもおすすめとのことです\n",
      "\n",
      "\n",
      "気になる男子と仲良くなろうと思って、勇気を振り絞って「今日、一緒に帰ろうよ」と言ったところで「ごめん、今日はちょっと用事があって」という返事が返ってきたら哀しいですよね。今回は、何人かの恋愛上手な女子に「気になる彼と仲良くなるための4つの鉄則」についてお話をお聞きしてきました。さっそくご紹介しましょう！■１．一緒に「帰る」ではなく「行く」「一緒に帰ろうと言っても、ちょっと仲良くなりづらいと思います。『帰ろう』というのは、なんとなく言いやすい言葉かもしれませんが、『一緒に行こう』がベターでは？　一緒にごはんに行こうとか、一緒に図書館に行こうとか」（28歳／モデル）\n",
      "つまり彼と共通の目的を持って行動しましょうということですよね。夜のお店の「同伴」とおなじです。気になる男子と同伴すれば、ごはんのあとに「一緒にお店に行く」という「共通の目的」が生まれます。帰ろう、つまり「アフターしよう」だと、アフターのあとどうしていいのかわからないので、お互いにオロオロ、ドキドキするだけです。■２．一緒にごはんを食べる「気になる彼と仲良くなろうと思えば、ランチでもいいので、絶対に何回も一緒にごはんを食べるべきです」（28歳／役員秘書）ドキドキしながら、なにを食べているのかわからないようなごはんであっても、絶対に何回も一緒にごはんを食べるべきです。ごはんって、ふたりの仲を理屈抜きにかなり親密にしてくれますよ。この「理屈抜きに」というのが、一緒にごはんを食べるという行為の素晴らしさです。■３．親族・家族の話をする「気になる彼と仲良くしたいと思えば、家族とか親戚の話をするといいです」（27歳／看護師）こちらも「理屈抜きに」仲良くなれる行為です。お互いに会っているときの相手しか知らないわけですが、家族や親戚の話というのは、いわばその人のルーツなわけです。共有することで相手の事を深く知ったような気になれますし、距離も縮まる、というわけです。■４．過去の話をする「気になる彼と仲良くなりたければ、高校時代のこととか、前職のこととか、とにかくじぶんの過去をお話するといいです」（28歳／受付）過去の話を聞きつつ、人は「じぶんとの共通点」を探しているものです。ああ、この女子も28歳にして3回も転職しているんだ……おれとおなじだ……という具合です。仲良くなるというのは、じぶんと他者との共通点を見出すことができたということですから、どんどん過去の話をしてみてはいかがでしょうか。男子はこのへんを勝手に拡大解釈して、過去の「自慢話」をしちゃうから困ったものです。■おわりにいかがでしたか？4つを並べてみると、どれも簡単なテクニックですよね。いつもおなじことを言うようで恐縮ですが、こういうテクニックって、恋愛上手な女子はもうほとんど本能的にやっていたりします。恋愛上手な女子って、「彼と共通の目的を持つって、それ、いったいどういうことだろう」なんて、むずかしくアタマで考えていないものです。だからあなたも、今日、気になる彼に「仕事が終わったら一緒にごはんに行こうよ」と言ってみてはいかがでしょうか。案ずるより産むが易しです。（ひとみしょう／ライター）（ハウコレ編集部）\n",
      "              \n",
      "            \n"
     ]
    }
   ],
   "source": [
    "index = 1\n",
    "target_data = body_data.iloc[index]\n",
    "article_id = target_data['article_id']\n",
    "body_text = target_data['text']\n",
    "summary_original_texts = summary_data.query(f'article_id == {article_id}', engine='python')['text'].tolist()\n",
    "\n",
    "print('\\n'.join(summary_original_texts))\n",
    "print()\n",
    "print(body_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5c34608-17c6-4eb8-8646-c3c263fddae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE = 'japanese'\n",
    "SENTENCES_COUNT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1fcfde6-5f0b-47c8-b2b8-07d7ea4bd2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "■３．親族・家族の話をする「気になる彼と仲良くしたいと思えば、家族とか親戚の話をするといいです」（27歳／看護師）こちらも「理屈抜きに」仲良くなれる行為です。\n",
      "■４．過去の話をする「気になる彼と仲良くなりたければ、高校時代のこととか、前職のこととか、とにかくじぶんの過去をお話するといいです」（28歳／受付）過去の話を聞きつつ、人は「じぶんとの共通点」を探しているものです。\n",
      "仲良くなるというのは、じぶんと他者との共通点を見出すことができたということですから、どんどん過去の話をしてみてはいかがでしょうか。\n"
     ]
    }
   ],
   "source": [
    "parser = PlaintextParser.from_string(body_text, Tokenizer(LANGUAGE))\n",
    "stemmer = Stemmer(LANGUAGE)\n",
    "summarizer = LexRankSummarizer(stemmer)\n",
    "\n",
    "for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7af34a23-ab1f-47f6-a260-4ab2a9b606c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "気になる男子と仲良くなろうと思って、勇気を振り絞って「今日、一緒に帰ろうよ」と言ったところで「ごめん、今日はちょっと用事があって」という返事が返ってきたら哀しいですよね。\n",
      "■２．一緒にごはんを食べる「気になる彼と仲良くなろうと思えば、ランチでもいいので、絶対に何回も一緒にごはんを食べるべきです」（28歳／役員秘書）ドキドキしながら、なにを食べているのかわからないようなごはんであっても、絶対に何回も一緒にごはんを食べるべきです。\n",
      "■４．過去の話をする「気になる彼と仲良くなりたければ、高校時代のこととか、前職のこととか、とにかくじぶんの過去をお話するといいです」（28歳／受付）過去の話を聞きつつ、人は「じぶんとの共通点」を探しているものです。\n"
     ]
    }
   ],
   "source": [
    "parser = PlaintextParser.from_string(body_text, Tokenizer(LANGUAGE))\n",
    "stemmer = Stemmer(LANGUAGE)\n",
    "summarizer = TextRankSummarizer(stemmer)\n",
    "\n",
    "for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ad2518f-13e0-4bd8-9615-0da32d6d9b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共有することで相手の事を深く知ったような気になれますし、距離も縮まる、というわけです。\n",
      "4つを並べてみると、どれも簡単なテクニックですよね。\n",
      "いつもおなじことを言うようで恐縮ですが、こういうテクニックって、恋愛上手な女子はもうほとんど本能的にやっていたりします。\n"
     ]
    }
   ],
   "source": [
    "parser = PlaintextParser.from_string(body_text, Tokenizer(LANGUAGE))\n",
    "stemmer = Stemmer(LANGUAGE)\n",
    "summarizer = LsaSummarizer(stemmer)\n",
    "\n",
    "for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edf20ff-424e-4ab0-b294-69accdd791e0",
   "metadata": {},
   "source": [
    "## 要約手法の評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ce99a75-1008-4cd6-86a8-556db693ace5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(summarizer, text, LANGUAGE, SENTENCES_COUNT):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(LANGUAGE))\n",
    "    summary_result_texts = []\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        summary_result_texts.append(sentence.__str__())\n",
    "    return summary_result_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b7bad32-50e6-4369-b243-2510fe9e9ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = RougeCalculator(stopwords=True, lang=\"ja\")\n",
    "bleu = BLEUCalculator(lang=\"ja\")\n",
    "stemmer = Stemmer(LANGUAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8ed6f09-b451-46d5-8f1a-45542fd5757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_dict = {\n",
    "    'lsa': LsaSummarizer,\n",
    "    'text_rank': TextRankSummarizer,\n",
    "    'lex_rank': LexRankSummarizer\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f33a053a-a373-4f16-9aa3-8ac0b8a063d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13462/13462 [21:14<00:00, 10.56it/s] \n"
     ]
    }
   ],
   "source": [
    "summary_results = []\n",
    "for i in tqdm(range(len(body_data))):\n",
    "\n",
    "    target_data = body_data.iloc[i]\n",
    "    article_id = target_data['article_id']\n",
    "    body_text = target_data['text']\n",
    "    summary_original_texts = summary_data.query(f'article_id == {article_id}', engine='python')['text'].tolist()\n",
    "\n",
    "    for summarize_name, Summarizer in summarize_dict.items():\n",
    "\n",
    "        summarizer = Summarizer(stemmer)\n",
    "\n",
    "        summary_result_texts = summarize_text(\n",
    "            summarizer=summarizer, text=body_text, LANGUAGE=LANGUAGE, SENTENCES_COUNT=SENTENCES_COUNT\n",
    "        )\n",
    "\n",
    "        summary_result = ''.join(summary_result_texts)\n",
    "        summary_original = ''.join(summary_original_texts)\n",
    "\n",
    "        rouge_1 = rouge.rouge_n(\n",
    "            summary=summary_result,\n",
    "            references=summary_original,\n",
    "            n=1\n",
    "        )\n",
    "\n",
    "        rouge_l = rouge.rouge_l(\n",
    "            summary=summary_result,\n",
    "            references=summary_original\n",
    "        )\n",
    "\n",
    "        bleu_score = bleu.bleu(summary_result, summary_original)\n",
    "\n",
    "        summary_results.append([article_id, summarize_name, rouge_1, rouge_l, bleu_score])\n",
    "\n",
    "summary_results = pd.DataFrame(\n",
    "    summary_results,\n",
    "    columns=['article_id', 'summarize_name', 'rouge_1', 'rouge_l', 'bleu_score']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c84309a9-4f39-4bb0-9c21-60217a6deec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">mean</th>\n",
       "      <th colspan=\"3\" halign=\"left\">median</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summarize_name</th>\n",
       "      <th>lex_rank</th>\n",
       "      <th>lsa</th>\n",
       "      <th>text_rank</th>\n",
       "      <th>lex_rank</th>\n",
       "      <th>lsa</th>\n",
       "      <th>text_rank</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>algorithm_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rank_bleu</th>\n",
       "      <td>1.819158</td>\n",
       "      <td>2.048655</td>\n",
       "      <td>2.132187</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_rouge1</th>\n",
       "      <td>1.770316</td>\n",
       "      <td>2.011105</td>\n",
       "      <td>2.218578</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_rougel</th>\n",
       "      <td>1.823800</td>\n",
       "      <td>2.067449</td>\n",
       "      <td>2.108751</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mean                       median               \n",
       "summarize_name  lex_rank       lsa text_rank lex_rank  lsa text_rank\n",
       "algorithm_name                                                      \n",
       "rank_bleu       1.819158  2.048655  2.132187      2.0  2.0       2.0\n",
       "rank_rouge1     1.770316  2.011105  2.218578      2.0  2.0       2.0\n",
       "rank_rougel     1.823800  2.067449  2.108751      2.0  2.0       2.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_data = summary_results.copy()\n",
    "plot_data['rank_rouge1'] = plot_data.groupby('article_id')['rouge_1'].rank(ascending=False)\n",
    "plot_data['rank_rougel'] = plot_data.groupby('article_id')['rouge_l'].rank(ascending=False)\n",
    "plot_data['rank_bleu'] = plot_data.groupby('article_id')['bleu_score'].rank(ascending=False)\n",
    "\n",
    "plot_data = plot_data[['article_id', 'summarize_name', 'rank_rouge1', 'rank_rougel', 'rank_bleu']].set_index(\n",
    "    ['article_id', 'summarize_name']\n",
    ").stack().reset_index()\n",
    "plot_data.columns = ['article_id', 'summarize_name', 'algorithm_name', 'rank']\n",
    "plot_data = plot_data.groupby(['algorithm_name', 'summarize_name']).agg({\n",
    "    'rank': ['mean', 'median']\n",
    "})\n",
    "plot_data.columns = list(map(lambda x: x[1], plot_data.columns))\n",
    "plot_data = plot_data.reset_index().pivot_table(\n",
    "    index='algorithm_name',\n",
    "    columns='summarize_name'\n",
    ")\n",
    "plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ce35d6a-bac9-4ebe-8279-ec7d65a335ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"5\" halign=\"left\">data_rate</th>\n",
       "      <th colspan=\"5\" halign=\"left\">n_data</th>\n",
       "      <th colspan=\"5\" halign=\"left\">total_n_data</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>1.0</th>\n",
       "      <th>1.5</th>\n",
       "      <th>2.0</th>\n",
       "      <th>2.5</th>\n",
       "      <th>3.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>1.5</th>\n",
       "      <th>2.0</th>\n",
       "      <th>2.5</th>\n",
       "      <th>3.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>1.5</th>\n",
       "      <th>2.0</th>\n",
       "      <th>2.5</th>\n",
       "      <th>3.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>algorithm_name</th>\n",
       "      <th>summarize_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">rank_bleu</th>\n",
       "      <th>lex_rank</th>\n",
       "      <td>0.347274</td>\n",
       "      <td>0.028228</td>\n",
       "      <td>0.432402</td>\n",
       "      <td>0.023102</td>\n",
       "      <td>0.168994</td>\n",
       "      <td>4675</td>\n",
       "      <td>380</td>\n",
       "      <td>5821</td>\n",
       "      <td>311</td>\n",
       "      <td>2275</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lsa</th>\n",
       "      <td>0.269202</td>\n",
       "      <td>0.018125</td>\n",
       "      <td>0.375279</td>\n",
       "      <td>0.020948</td>\n",
       "      <td>0.316446</td>\n",
       "      <td>3624</td>\n",
       "      <td>244</td>\n",
       "      <td>5052</td>\n",
       "      <td>282</td>\n",
       "      <td>4260</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_rank</th>\n",
       "      <td>0.175680</td>\n",
       "      <td>0.025553</td>\n",
       "      <td>0.462561</td>\n",
       "      <td>0.031125</td>\n",
       "      <td>0.305081</td>\n",
       "      <td>2365</td>\n",
       "      <td>344</td>\n",
       "      <td>6227</td>\n",
       "      <td>419</td>\n",
       "      <td>4107</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">rank_rouge1</th>\n",
       "      <th>lex_rank</th>\n",
       "      <td>0.370747</td>\n",
       "      <td>0.026445</td>\n",
       "      <td>0.435448</td>\n",
       "      <td>0.026148</td>\n",
       "      <td>0.141212</td>\n",
       "      <td>4991</td>\n",
       "      <td>356</td>\n",
       "      <td>5862</td>\n",
       "      <td>352</td>\n",
       "      <td>1901</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lsa</th>\n",
       "      <td>0.272842</td>\n",
       "      <td>0.019759</td>\n",
       "      <td>0.403060</td>\n",
       "      <td>0.021022</td>\n",
       "      <td>0.283316</td>\n",
       "      <td>3673</td>\n",
       "      <td>266</td>\n",
       "      <td>5426</td>\n",
       "      <td>283</td>\n",
       "      <td>3814</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_rank</th>\n",
       "      <td>0.148641</td>\n",
       "      <td>0.025553</td>\n",
       "      <td>0.429580</td>\n",
       "      <td>0.032462</td>\n",
       "      <td>0.363765</td>\n",
       "      <td>2001</td>\n",
       "      <td>344</td>\n",
       "      <td>5783</td>\n",
       "      <td>437</td>\n",
       "      <td>4897</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">rank_rougel</th>\n",
       "      <th>lex_rank</th>\n",
       "      <td>0.342148</td>\n",
       "      <td>0.027559</td>\n",
       "      <td>0.437825</td>\n",
       "      <td>0.025479</td>\n",
       "      <td>0.166989</td>\n",
       "      <td>4606</td>\n",
       "      <td>371</td>\n",
       "      <td>5894</td>\n",
       "      <td>343</td>\n",
       "      <td>2248</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lsa</th>\n",
       "      <td>0.255534</td>\n",
       "      <td>0.020725</td>\n",
       "      <td>0.379958</td>\n",
       "      <td>0.020874</td>\n",
       "      <td>0.322909</td>\n",
       "      <td>3440</td>\n",
       "      <td>279</td>\n",
       "      <td>5115</td>\n",
       "      <td>281</td>\n",
       "      <td>4347</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_rank</th>\n",
       "      <td>0.192096</td>\n",
       "      <td>0.028228</td>\n",
       "      <td>0.449636</td>\n",
       "      <td>0.030159</td>\n",
       "      <td>0.299881</td>\n",
       "      <td>2586</td>\n",
       "      <td>380</td>\n",
       "      <td>6053</td>\n",
       "      <td>406</td>\n",
       "      <td>4037</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "      <td>13462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              data_rate                                \\\n",
       "rank                                1.0       1.5       2.0       2.5   \n",
       "algorithm_name summarize_name                                           \n",
       "rank_bleu      lex_rank        0.347274  0.028228  0.432402  0.023102   \n",
       "               lsa             0.269202  0.018125  0.375279  0.020948   \n",
       "               text_rank       0.175680  0.025553  0.462561  0.031125   \n",
       "rank_rouge1    lex_rank        0.370747  0.026445  0.435448  0.026148   \n",
       "               lsa             0.272842  0.019759  0.403060  0.021022   \n",
       "               text_rank       0.148641  0.025553  0.429580  0.032462   \n",
       "rank_rougel    lex_rank        0.342148  0.027559  0.437825  0.025479   \n",
       "               lsa             0.255534  0.020725  0.379958  0.020874   \n",
       "               text_rank       0.192096  0.028228  0.449636  0.030159   \n",
       "\n",
       "                                        n_data                        \\\n",
       "rank                                3.0    1.0  1.5   2.0  2.5   3.0   \n",
       "algorithm_name summarize_name                                          \n",
       "rank_bleu      lex_rank        0.168994   4675  380  5821  311  2275   \n",
       "               lsa             0.316446   3624  244  5052  282  4260   \n",
       "               text_rank       0.305081   2365  344  6227  419  4107   \n",
       "rank_rouge1    lex_rank        0.141212   4991  356  5862  352  1901   \n",
       "               lsa             0.283316   3673  266  5426  283  3814   \n",
       "               text_rank       0.363765   2001  344  5783  437  4897   \n",
       "rank_rougel    lex_rank        0.166989   4606  371  5894  343  2248   \n",
       "               lsa             0.322909   3440  279  5115  281  4347   \n",
       "               text_rank       0.299881   2586  380  6053  406  4037   \n",
       "\n",
       "                              total_n_data                              \n",
       "rank                                   1.0    1.5    2.0    2.5    3.0  \n",
       "algorithm_name summarize_name                                           \n",
       "rank_bleu      lex_rank              13462  13462  13462  13462  13462  \n",
       "               lsa                   13462  13462  13462  13462  13462  \n",
       "               text_rank             13462  13462  13462  13462  13462  \n",
       "rank_rouge1    lex_rank              13462  13462  13462  13462  13462  \n",
       "               lsa                   13462  13462  13462  13462  13462  \n",
       "               text_rank             13462  13462  13462  13462  13462  \n",
       "rank_rougel    lex_rank              13462  13462  13462  13462  13462  \n",
       "               lsa                   13462  13462  13462  13462  13462  \n",
       "               text_rank             13462  13462  13462  13462  13462  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_data = summary_results.copy()\n",
    "plot_data['rank_rouge1'] = plot_data.groupby('article_id')['rouge_1'].rank(ascending=False)\n",
    "plot_data['rank_rougel'] = plot_data.groupby('article_id')['rouge_l'].rank(ascending=False)\n",
    "plot_data['rank_bleu'] = plot_data.groupby('article_id')['bleu_score'].rank(ascending=False)\n",
    "\n",
    "plot_data = plot_data[['article_id', 'summarize_name', 'rank_rouge1', 'rank_rougel', 'rank_bleu']].set_index(\n",
    "    ['article_id', 'summarize_name']\n",
    ").stack().reset_index()\n",
    "plot_data.columns = ['article_id', 'summarize_name', 'algorithm_name', 'rank']\n",
    "plot_data = plot_data.groupby(['algorithm_name', 'summarize_name', 'rank'])['article_id'].count().reset_index().rename(\n",
    "    columns={'article_id': 'n_data'}\n",
    ")\n",
    "plot_data['total_n_data'] = plot_data.groupby(['algorithm_name', 'summarize_name'])['n_data'].transform('sum')\n",
    "plot_data = plot_data.assign(data_rate=lambda x: x.n_data / x.total_n_data).pivot_table(\n",
    "    index=['algorithm_name', 'summarize_name'],\n",
    "    columns='rank'\n",
    ")\n",
    "plot_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c354f7-4b8a-4504-8327-68a6cae1064b",
   "metadata": {},
   "source": [
    "## Extracting Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2df5940-2049-45e7-99f3-b1be06e635ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "class TextRank4Keyword():\n",
    "    \"\"\"Extract keywords from text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 10 # iteration steps\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "\n",
    "    \n",
    "    def set_stopwords(self, stopwords):  \n",
    "        \"\"\"Set stop words\"\"\"\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "    \n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                # Store words only with cadidate POS tag\n",
    "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "        \n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "    \n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "        \n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "    \n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "            \n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "        \n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "        \n",
    "        return g_norm\n",
    "\n",
    "    \n",
    "    def get_keywords(self, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        for i, (key, value) in enumerate(node_weight.items()):\n",
    "            print(key + ' - ' + str(value))\n",
    "            if i > number:\n",
    "                break\n",
    "        \n",
    "        \n",
    "    def analyze(self, text, \n",
    "                candidate_pos=['NOUN', 'PROPN'], \n",
    "                window_size=4, lower=False, stopwords=list()):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "        \n",
    "        # Set stop words\n",
    "        self.set_stopwords(stopwords)\n",
    "        \n",
    "        # Pare text by spaCy\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n",
    "        \n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "        \n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "        \n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "        \n",
    "        # Initionlization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "        \n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "        \n",
    "        self.node_weight = node_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29dc2356-7499-4aa5-aaea-8c1d114ebb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "science - 1.7521671497168656\n",
      "fiction - 1.7168390247168657\n",
      "China - 1.4722488091138661\n",
      "Earth - 1.4143046707606364\n",
      "Wandering - 1.1038853114478115\n",
      "tone - 1.0971675275482093\n",
      "fans - 1.0971675275482093\n",
      "weekend - 1.0310056818181819\n",
      "America - 1.0260545033670034\n",
      "North - 1.0076557239057238\n",
      "throwback - 1.0019926346801344\n",
      "budget - 1.0014987038950105\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "The Wandering Earth, described as China’s first big-budget science fiction thriller, quietly made it onto screens at AMC theaters in North America this weekend, and it shows a new side of Chinese filmmaking — one focused toward futuristic spectacles rather than China’s traditionally grand, massive historical epics. At the same time, The Wandering Earth feels like a throwback to a few familiar eras of American filmmaking. While the film’s cast, setting, and tone are all Chinese, longtime science fiction fans are going to see a lot on the screen that reminds them of other movies, for better or worse.\n",
    "'''\n",
    "\n",
    "tr4w = TextRank4Keyword()\n",
    "tr4w.analyze(text, candidate_pos = ['NOUN', 'PROPN'], window_size=4, lower=False)\n",
    "tr4w.get_keywords(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a5e8b6-fe07-49f8-854c-adfd51745042",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
